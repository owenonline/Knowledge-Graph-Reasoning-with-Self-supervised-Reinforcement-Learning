# Knowledge-Graph-Reasoning-with-Self-supervised-Reinforcement-Learning
Reinforcement learning (RL) is an effective method to find reasoning pathways in incomplete knowledge graphs (KGs). To overcome the challenges of sparse rewards and the explore-exploit dilemma, a self-supervised pretraining method is proposed to warm up the policy network before the RL training stage. The seeding paths used in the supervised pretraining stage are generated by searching the 3-hop neighborhoods of start entities in a subset of training facts. Our self-supervised RL (SSRL) method with partial labels combines the fast learning speed of RL and wide coverage of SL. We adopt two RL architectures, i.e., MINERVA and MultiHopKG as our baseline RL models and experimentally show that our SSRL model consistently outperforms both baselines on all Hits@k and mean reciprocal rank (MRR) metrics on four large benchmark KG datasets. We also show that our SSRL model (either SS-MINERVA or SS-MultiHopKG) meets or exceeds current state-of-the-art results for all of these KG reasoning tasks.
